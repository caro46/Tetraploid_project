## [Allpaths](http://software.broadinstitute.org/allpaths-lg/blog/), de novo assembly (possibility of adding a Reference)
STUCTURE: `REFERENCE/DATA/RUN/ASSEMBLIES/SUBDIR`.

INPUTS: sequenced reads, their quality scores and information concerning their pairing (`.fastb` + `.qualb` obtained with `PrepareAllPathsInputs.pl`) + ploidy files (only `1=haploid`, `2=diploid`).

In the ALLPATHS it is advice to use a **reference genome** if one is available to help building the genome: using *X.tropicalis*.
DATA directories = sets of reads to assemble.
Arguments to give 

> PRE – the root directory in which the ALLPATHS pipeline directory will be created. (= '/work/ben/Mellotropicalis_corrected_data')

> REFERENCE_NAME – the REFERENCE (organism) directory name 

> DATA_SUBDIR – the DATA (project) directory name

> RUN – the RUN (assembly pre ‐ processing) directory name (='allpaths')

> SUBDIR – the SUBDIR (assembly) directory name

#### Preparing the data
We have paired-end libraries. We need to rename the files `Ben_Evans_BJE3652_180_BP_Library_2nd_Sequence_09082015_NoIndex_L008_R1_011_trim_paired.fastq.gz` needs to become `Ben_Evans_BJE3652_180_BP_Library_2nd_Sequence_09082015_NoIndex_L008_R1_011_trim_paired.A.fastq.gz` .
```bash
#!/bin/bash

#for file in *R1*.fastq.gz; do
#    mv "$file" "${file/.fastq.gz/.A.fastq.gz}"
#done

#for file2 in *R2*.fastq.gz; do
#    mv "$file2" "${file2/.fastq.gz/.B.fastq.gz}"
#done

for file in *R1*.fastq.gz; do
    mv "$file" "${file/_R1/}"
done

for file2 in *R2*.fastq.gz; do
    mv "$file2" "${file2/_R2/}"
done
```
```
(on scratch)
module load allpathslg
PrepareAllPathsInputs.pl DATA_DIR=/scratch/ben/allpath_assembly/X_mellotropicalis/data PLOIDY=2 IN_GROUPS_CSV=/scratch/ben/allpath_assembly/X_mellotropicalis/in_groups.csv IN_LIBS_CSV=/scratch/ben/allpath_assembly/X_mellotropicalis/in_libs.csv 
```
`PLOIDY` only 1 (haploid) or 2 (diploid) no other ploidy level tested.
The `DATA_DIR` needs to contain the `in_groups.csv` & `in_libs.csv` files.
The reference genome must be supplied as two files: genome.fasta and genome.fastb. (Run `Fasta2Fastb`) - 1st we will do it without any ref. seq (`Run1`).

`in_groups.csv`
```
group_name,library_name,file_name
6kb,Illumina_6kb,/scratch/ben/allpath_assembly/X_mellotropicalis/data/Ben-Evans-BJE3652-6kb_NoIndex_*.fastq.gz
1kb,Illumina_1kb,/scratch/ben/allpath_assembly/X_mellotropicalis/data/BenEvansBJE3652_1000bp_Library*.fastq.gz
10kb,Illumina_10kb,/scratch/ben/allpath_assembly/X_mellotropicalis/data/Ben_Evans_BJE3652_10kb_Mate_Pair_Library*.fastq.gz
180bp_2,Illumina_180bp_2,/scratch/ben/allpath_assembly/X_mellotropicalis/data/Ben_Evans_BJE3652_180_BP_Library_2nd_Sequence*.fastq.gz
180bp_1,Illumina_180bp,/scratch/ben/allpath_assembly/X_mellotropicalis/data/BenEvansBJE3652_180bp_Library_GTTTCG*.fastq.gz
400bp,Illumina_400bp,/scratch/ben/allpath_assembly/X_mellotropicalis/data/BenEvansBJE3652_400bp*.fastq.gz
6kb_2,Illumina_6kb_2,/scratch/ben/allpath_assembly/X_mellotropicalis/data/Ben_Evans_BJE3652_6kb_2nd_Sequencing*.fastq.gz
```
We need to have a separate group for each pair of reads:
```perl
#!/usr/bin/perl

# This script is to produce the input file
# for allpaths: "in_groups.csv" 

use strict;
use warnings;
my @files;
my @files_R1_180_1;
my @files_R1_180_2;
my @files_R1_400;
my @files_R1_1000;
my @files_R1_6000_1;
my @files_R1_6000_2;
my @files_R1_10000;
my $path_to_data;
my $output = "in_groups.csv";

# Use the open() function to create the file.
unless(open FILE, '>'.$output) {
    # Die with error message 
    # if we can't open it.
    die "\nUnable to create $output\n";
}

$path_to_data = "/scratch/ben/allpath_assembly/X_mellotropicalis/data/";
print FILE "group_name,library_name,file_name","\n";

##1st library
#180bp_1,Illumina_180bp,/scratch/ben/allpath_assembly/X_mellotropicalis/data/BenEvansBJE3652_180bp_Library_GTTTCG*.fastq.gz

@files_R1_180_1 = glob($path_to_data."BenEvansBJE3652_180bp_Library_GTTTCG*A.fastq.gz");
my $y;
for ($y=0; $y<=$#files_R1_180_1; $y ++) {
    print FILE "180bp_1_",$y,",Illumina_180bp,", $files_R1_180_1[$y],"\n";
}

##2nd library
#180bp_2,Illumina_180bp_2,/scratch/ben/allpath_assembly/X_mellotropicalis/data/Ben_Evans_BJE3652_180_BP_Library_2nd_Sequence*.fastq.gz

@files_R1_180_2 = glob($path_to_data."Ben_Evans_BJE3652_180_BP_Library_2nd_Sequence*A.fastq.gz");
for ($y=0; $y<=$#files_R1_180_2; $y ++) {
    print FILE "180bp_2_",$y,",Illumina_180bp_2,", $files_R1_180_2[$y],"\n";
}

##3rd library
#400bp,Illumina_400bp,/scratch/ben/allpath_assembly/X_mellotropicalis/data/BenEvansBJE3652_400bp*.fastq.gz

@files_R1_400 = glob($path_to_data."BenEvansBJE3652_400bp*A.fastq.gz");
for ($y=0; $y<=$#files_R1_400; $y ++) {
    print FILE "400bp_",$y,",Illumina_400bp,", $files_R1_400[$y],"\n";
}

##4th library
#1kb,Illumina_1kb,/scratch/ben/allpath_assembly/X_mellotropicalis/data/BenEvansBJE3652_1000bp_Library*.fastq.gz

@files_R1_1000 = glob($path_to_data."BenEvansBJE3652_1000bp_Library*A.fastq.gz");
for ($y=0; $y<=$#files_R1_1000; $y ++) {
    print FILE "1kb_",$y,",Illumina_1kb,", $files_R1_1000[$y],"\n";
}

##5th library
#6kb,Illumina_6kb,/scratch/ben/allpath_assembly/X_mellotropicalis/data/Ben-Evans-BJE3652-6kb_NoIndex_*.fastq.gz

@files_R1_6000_1 = glob($path_to_data."Ben-Evans-BJE3652-6kb_NoIndex_*A.fastq.gz");
for ($y=0; $y<=$#files_R1_6000_1; $y ++) {
    print FILE "6kb_1",$y,",Illumina_6kb,", $files_R1_6000_1[$y],"\n";
}

##6th library
#6kb_2,Illumina_6kb_2,/scratch/ben/allpath_assembly/X_mellotropicalis/data/Ben_Evans_BJE3652_6kb_2nd_Sequencing*.fastq.gz

@files_R1_6000_2 = glob($path_to_data."Ben_Evans_BJE3652_6kb_2nd_Sequencing*A.fastq.gz");
for ($y=0; $y<=$#files_R1_6000_2; $y ++) {
    print FILE "6kb_2_",$y,",Illumina_6kb_2,", $files_R1_6000_2[$y],"\n";
}

##7th library
#10kb,Illumina_10kb,/scratch/ben/allpath_assembly/X_mellotropicalis/data/Ben_Evans_BJE3652_10kb_Mate_Pair_Library*.fastq.gz
@files_R1_10000 = glob($path_to_data."Ben_Evans_BJE3652_10kb_Mate_Pair_Library*A.fastq.gz");
for ($y=0; $y<=$#files_R1_10000; $y ++) {
    print FILE "10kb_",$y,",Illumina_10kb,", $files_R1_10000[$y],"\n";
}

#Change the "A" into "*"
# close the file.
close FILE;

my $commandline;
my $status;
$commandline = "sed -i -e s/A.fastq.gz/\*.fastq.gz/g ".$output;
print $commandline;
$status=system($commandline);

exit 0;
```

`in_libs.csv`
```
library_name,project_name,organism_name,type,paired,frag_size,frag_stddev,insert_size,insert_stddev,read_orientation,genomic_start,genomic_end
Illumina_180bp,Mellotropicalis_poject,Xenopus_mellotropicalis,fragment,1,90,60,,,inward,0,0
Illumina_180bp_2,Mellotropicalis_poject,Xenopus_mellotropicalis,fragment,1,90,60,,,inward,0,0
Illumina_400bp,Mellotropicalis_poject,Xenopus_mellotropicalis,fragment,1,90,60,,,inward,0,0
Illumina_1kb,Mellotropicalis_poject,Xenopus_mellotropicalis,fragment,1,90,60,,,inward,0,0
Illumina_6kb,Mellotropicalis_poject,Xenopus_mellotropicalis,jumping,1,,,6000,500,outward,0,0
Illumina_6kb_2,Mellotropicalis_poject,Xenopus_mellotropicalis,jumping,1,,,6000,500,outward,0,0
Illumina_10kb,Mellotropicalis_poject,Xenopus_mellotropicalis,jumping,1,,,10000,500,outward,0,0
```
Some issues with the memory. 

From one of the `.log` file:
```
Sun Nov 20 13:16:05 2016 (MPF): Merging.

Fatal error (pid=19712) at Sun Nov 20 13:24:38 2016:
Attempt to write 16384 bytes to /scratch/ben/allpath_assembly/X_mellotropicalis/data/read_cache/180b
p_1_0.tmp/all.qualb failed after writing 0 bytes: No space left on device [errno=28].


Sun Nov 20 13:24:38 2016.  Abort.  Stopping.

Generating a backtrace...

Dump of stack:

0. CRD::exit(int), in ??:0
1. FileWriter::write(void const*, unsigned long) const, in FileWriter.cc:53
2. writeBuf, in BinaryStream.h:158
3. writeArray<long unsigned int>, in BinaryStream.h:135
4. write<long unsigned int>, in BinaryStream.h:84
5. FeudalFileWriter::close(), in FeudalFileWriter.cc:78
```

Try again with deleting all the intermediate files from previous attempts (in case some of them weren't overwritten). If still issues see what we can do with some options that don't appear in the Manual:
```
Nov 21 08:35:17 2016 run on localhost (pid=25439), last modified Nov 17 15:29:25 2016
/opt/sharcnet/allpathslg/52488/bin/ConvertToFastbQualb.pl                      \
    PICARD_TOOLS_DIR=                                                          \
    TMP_DIR=/scratch/ben/allpath_assembly/X_mellotropicalis/data/read_cache/picard_tmp \
    DATA_FILE=/scratch/ben/allpath_assembly/X_mellotropicalis/data/BenEvansBJE3652_180bp_Library_GTTTCG_L001_004_trim_paired.cor.*.fastq.gz \
    PAIRED=1                                                                   \
    OUT_HEAD=/scratch/ben/allpath_assembly/X_mellotropicalis/data/read_cache/180bp_1_3 \
    REVERSE_READS=False TRIM_START=0 TRIM_END=0 INCLUDE_NON_PF_READS=1         \
    PHRED_64=0 FORCE_PHRED=0 REVERT_SAM=0 OVERWRITE=0 SAVE_COMPRESSED_FASTQ=0  \
    SAVE_INTERMEDIATES=0 JAVA_MEM_GB=8 DRY_RUN=0 VERBOSE=1
```
From the [Allpaths website](http://software.broadinstitute.org/allpaths-lg/blog/?page_id=215):

*C2. How much memory does ALLPATHS-LG require?*

*Peak usage is roughly 1.7 bytes per read base, at least for mammalian-size genomes. If you find that memory usage is much higher than this, we will try to help.  There are many places in the algorithm where memory usage could be reduced.*

Potential memory issues considering the number of intermediate and "final" files produced even when just running the assembly on the test data -> a looooot of files will be created in new directories also created by the program. No information if they are necessary for running or just for recovery, if memory issues, try moving intermediate files produced by the 1st command to freezer and then run the assembly.

On a [discussion website](https://www.biostars.org/p/8973/), John St John (crocodilian assembly):

*The broad talks about some genomes >500MB it assembled posted on its allpaths-lg blog. I personally haven't had luck with it yet b/c not all of the modules respect the MAX_MEM_GB (or something similar) argument. I have a 1TB system to work on, but it is shared, so it crashes at one of the modules that looks at how much memory is available and tries to use it all. If you are on your own dedicated system with enough ram then you should be ok. They claim to be fixing that particular issue now so people on shared systems can use the program.*

*I used Allpaths LG to generate the preliminary assemblies in the Crocodilian genome announcement paper last year http://genomebiology.com/content/13/1/415. Using a combination overlap library, and a reasonable coverage 2kb insert library using a custom protocol that Nader Pourmand at UCSC developed, we were able to achieve a scaffold N50 of 106Kb, and a contig N50 of 28Kb. I have not been involved with the project recently, so I am not sure what the current state of the assemblies are, but Allpaths-LG did do a good job on that genome with the data we had available at the time.* 

It was ~5years ago but it seems to still be the case.

Try the permanent cache?
```
  CacheLibs.pl 
    CACHE_DIR=<CACHE_DIR> 
    IN_LIBS_CSV=in_libs.csv  
    ACTION=Add 
  CacheGroups.pl 
    CACHE_DIR=<CACHE_DIR> 
    PICARD_TOOLS_DIR=/opt/picard/bin  
    IN_GROUPS_CSV=in_groups.csv  
    TMP_DIR=/large-tmp 
    HOSTS=’2,3.host2,4.host3’ 
    ACTION=Add 
```
Try what the crocodile guy tried (even if it wasn't workin, maybe since then it has been fixed):
```
PrepareAllPathsInputs.pl DATA_DIR=/scratch/ben/allpath_assembly/X_mellotropicalis/data PLOIDY=2 IN_GROUPS_CSV=/scratch/ben/allpath_assembly/X_mellotropicalis/in_groups.csv IN_LIBS_CSV=/scratch/ben/allpath_assembly/X_mellotropicalis/in_libs.csv HOSTS=8 MAX_MEM_GB=400 
```
By specifying `MAX_MEM_GB=500` when re-running (no overwrite): new files (that previously failed) were created (ex. `6kb_2_2.fastb`) (was OK because some nodes with a lot of memory and nobody was running but when someone: crashed). Run on Iqaluk (before Wobbie) for the last files `6kb_10` to `6kb_19` without memory restriction. (Failed when used small values)

Looking on the internet, a lot of memory issues for people using this software for big genome:
[Here](http://seqanswers.com/forums/showthread.php?t=45543) - problem for a mouse genome running on a server -> ask sharcnet which node to select, only run on this one and alocating a lot (>500Gb) of memory? (no multi-threads?) 

Another one [here](http://seqanswers.com/forums/showthread.php?t=25729):
*After some profiling by using the collectl, I found that the last program running before my allpaths-lg job terminated is FindErrors. And the reason was FindErrors tried to allocate more memory than it is available on the machine for loading some files. So I reran my job on another machine that has 2TB memory and it worked. Hope this will help.*

Sounds like we will need help from Mark (sharcnet) and/or Brian... Interesting software that asks you to have a lot of data to have a lot of coverage with different insert size (minimum being one small and one big >2kb) but requires too much memory. Not sure how much we will need for this tetraploid genome and all the data we have (needed to try to solve the main issues of tetraploid genome: high heterozigosity & repeat elements)... And for the moment it is the 1st step, preparing the data.
We will probably needs the help from the software team who was very helpful for the crocodile genome. Memory issues were also reported [there](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4173024/) when they tried to incorporate PacBio sequences.

#### Solution
We previously wanted to use `/scratch` since there is no limit of space but almost full.... So come back on `/work` but need to check regularly if we are overquota. Need to either delete directly the apparently not necessary outputs of the 1st step or `mv` them to freezer until the complete assembly is done (max 1 month?) just in case. Run on `wobbie` (`wob101`) to have enough RAM. Not too many threats to not have competition for memory.
```
PrepareAllPathsInputs.pl DATA_DIR=/work/ben/Mellotropicalis_corrected_data/allpaths/data PLOIDY=2 IN_GROUPS_CSV=/work/ben/Mellotropicalis_corrected_data/allpaths/in_groups.csv IN_LIBS_CSV=/work/ben/Mellotropicalis_corrected_data/allpaths/in_libs.csv HOSTS=8 MAX_MEM_GB=400
```
Other issues: fragment files not created because `GROUP_READ_SIZES (vec<double>)` not specified. No explanation what it is in the manual. Because at the beginning we only need to give 2 files, the issue should come from one of them. Some people have noticed a bad conversion with excel to `.csv` with blank not having single space. When we run again still same error. So delete the previous files and run again. Only issue with the fragment part, no problem with the jumping libraries.

##### Try subsetting (see if memory issue)
- When runnning on only 1 couple of files (1 R1, 1 R2) for each library:`PrepareAllPathsInputs.pl` runs perfectly.
- When running on everything except the 2nd sequencing of 180pb: worked!
- Everything + half the 2nd sequencing of 180pb: not working
- Everything + 1/3 the 2nd sequencing of 180pb: not working
- Everything + 1/4 the 2nd sequencing of 180pb: not working
- Everything + 1/5 the 2nd sequencing of 180pb: not working
- Everything + 1/10 the 2nd sequencing of 180pb: not working (try without parallelization, and allowing more memory `500G` still not working even when don't specify the memory limit)
- Everything + 2 couples of the 2nd sequencing of 180pb: worked

Ok so it is definitely a memory issue. Let's go whithout the 180pb-2nd sequencing libraries and try the 2nd step (the assembly) to see if we will have a memory issue after.
```
PrepareAllPathsInputs.pl DATA_DIR=/work/ben/Mellotropicalis_corrected_data/allpaths/data PLOIDY=2 IN_GROUPS_CSV=/work/ben/Mellotropicalis_corrected_data/allpaths/in_groups_test_no_180_2.csv IN_LIBS_CSV=/work/ben/Mellotropicalis_corrected_data/allpaths/in_libs.csv HOSTS=8 MAX_MEM_GB=400
RunAllPathsLG PRE=/work/ben/Mellotropicalis_corrected_data REFERENCE_NAME=allpaths DATA_SUBDIR=data RUN=Run1_no_180_2 TARGETS=standard THREADS=8 
```
```
Giving up.  Here are some possible solutions:
- Run without other competing processes (if that's the problem).
- Run on a server having more memory, or reduce your input data amount.
- Consider using the MAX_MEM_GB or MEMORY_CHECK options (if available).
```
`You should not have specified a value for MAX_MEM_GB` for any value tried. Try with no limit, no threat.

#### Running the assembly
```
RunAllPathsLG
PRE=/work/ben/Mellotropicalis_corrected_data
REFERENCE_NAME=allpaths
DATA_SUBDIR=data
RUN=Run1
TARGETS=standard
#EVALUATION=standard #not for Run1 (don't use X. tropicalis as a reference)
THREADS=10
```
!!!! Differences with Abyss & SOAPdenovo: no need of specify a k-mer size (default `K=96`). No direct linked between k-mer size and read length. Allpaths uses multiple values when building the assembly. 


##On test data
```
module load allpathslg
PrepareAllPathsInputs.pl DATA_DIR=/scratch/ben/allpath_assembly/ALLPATHS-LG.test_genome/seq IN_GROUPS_CSV=/scratch/ben/allpath_assembly/ALLPATHS-LG.test_genome/seq/in_groups.csv IN_LIBS_CSV=/scratch/ben/allpath_assembly/ALLPATHS-LG.test_genome/seq/in_libs.csv
RunAllPathsLG PRE=/scratch/ben/allpath_assembly/ REFERENCE_NAME=ALLPATHS-LG.test_genome DATA_SUBDIR=seq RUN=myrun TARGETS=standard 
```
Checked with test files if really no overwritting of the files that were successfully produced (`.qualb` `.fastb`) in `read_cache`: no but produced new final files in the `seq` directory. -> so try to run multiple times for our data and see if we can produce the needed files for the ones that we previously couldn't. (no `overwrite` information of this step in the manual).

(`myrun` is created while running, needs a `ploidy` file in the DATA directory)
